{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Build an Image Captioning System with IBM watsonx and Granite](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you’ll explore how to use the IBM Granite 3.2 Vision model with IBM watsonx to perform multimodal tasks like image captioning and visual question answering using Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "<ul>\n",
    "    <li>\n",
    "        <a href=\"#toc1_\">Build an Image Captioning System with IBM watsonx and Granite 3.2 Vision</a>\n",
    "        <ul>\n",
    "            <li><a href=\"#Introduction\">Introduction</a></li>\n",
    "            <li><a href=\"#What-does-this-guided-project-do?\">What does this guided project do?</a></li>\n",
    "            <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "            <li>\n",
    "                <a href=\"#Background\">Background</a>\n",
    "                <ul>\n",
    "                    <li><a href=\"#What-is-large-language-model-(LLM)?\">What is large language model (LLM)?</a></li>\n",
    "                    <li><a href=\"#What-is-IBM-watsonx?\">What is IBM watsonx?</a></li>\n",
    "                    <li><a href=\"#What-is-IBM-Granite-3.2-Vision?\">What is IBM Granite 3.2 Vision?</a></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                <a href=\"#Setup\">Setup</a>\n",
    "                <ul>\n",
    "                    <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><a href=\"#watsonx-API-credentials-and-project_id\">watsonx API credentials and project_id</a></li>\n",
    "            <li><a href=\"#Image-preparation\">Image preparation</a></li>\n",
    "            <li>\n",
    "                <a href=\"#Work-with-large-language-models-on-watsonx.ai\">Work with large language models on watsonx.ai</a>\n",
    "                <ul>\n",
    "                    <li><a href=\"#Check-the-model-parameters\">Check the model parameters</a></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li><a href=\"#Initialize-the-model\">Initialize the model</a></li>\n",
    "            <li><a href=\"#Encode-the-image\">Encode the image</a></li>\n",
    "            <li><a href=\"#Multimodal-inference-function\">Multimodal inference function</a></li>\n",
    "            <li><a href=\"#Image-captioning\">Image captioning</a></li>\n",
    "            <li><a href=\"#Object-detection\">Object detection</a></li>\n",
    "            <li><a href=\"#Conclusion\">Conclusion</a></li>\n",
    "            <li>\n",
    "                <a href=\"#Exercises\">Exercises</a>\n",
    "            </li>\n",
    "            <li><a href=\"#Authors\">Authors</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Introduction](#toc0_)\n",
    "\n",
    "Visual content—like photos, screenshots, or charts—often contains important information that can be hard to interpret at a glance. Wouldn’t it be useful if an AI model could instantly describe what’s in an image, or answer questions about it?\n",
    "\n",
    "In this guided project, we’ll explore how to use a large multimodal language model to do exactly that. You'll use IBM's Granite 3.2 Vision model, integrated with IBM watsonx, to generate text responses based on visual inputs. From scene descriptions to answering specific questions, this model can help turn images into insights.\n",
    "\n",
    "## [What does this guided project do?](#toc0_)\n",
    "\n",
    "This project demonstrates how to:\n",
    "\n",
    "- Load image data from URLs.\n",
    "- Encode those images so they can be processed by a language model.\n",
    "- Use the IBM's Granite 3.2 Vision model through IBM watsonx to generate text responses based on each image.\n",
    "## [Objectives](#toc0_)\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "- Understand how to encode images for LLM-based visual processing.\n",
    "- Use the IBM's Granite 3.2 Vision model to describe or analyze images.\n",
    "- Interact with the IBM watsonx API to perform multimodal queries in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Background](#toc0_)\n",
    "\n",
    "### [What is large language model (LLM)?](#toc0_)\n",
    "\n",
    "[Large language models](https://www.ibm.com/think/topics/large-language-models?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Build+an+Image+Captioning+System+with+watsonx+and+Llama-v1_1745515774) are a category of foundation models that are trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks.\n",
    "\n",
    "### [What is IBM watsonx?](#toc0_)\n",
    "\n",
    "[IBM watsonx](https://www.ibm.com/watsonx?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Build+an+Image+Captioning+System+with+watsonx+and+Llama-v1_1745515774) is a suite of artificial intelligence (AI) tools and services that are designed to help developers build and deploy AI-driven applications. Watsonx provides a range of APIs and tools that make it easy to integrate AI capabilities into applications, including natural language processing, computer vision, and speech recognition.\n",
    "\n",
    "**Enterprises turn to watsonx because it is:**\n",
    "\n",
    "- **Open**: Based on open technologies that provide a variety of models to cover enterprise use cases and support compliance initiatives.\n",
    "- **Targeted**: Targeted to specific enterprise domains like HR, customer service, or IT operations to unlock new value.\n",
    "- **Trusted**: Designed with principles of transparency, responsibility, and governance so you can manage ethical and accuracy concerns.\n",
    "- **Empowering**: You can go beyond being an AI user and become an AI value creator, owning the value that your models create.\n",
    "\n",
    "### [What is IBM Granite 3.2 Vision?](#toc0_)\n",
    "\n",
    "[IBM's Granite 3.2 series](https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Build+an+Image+Captioning+System+with+watsonx+and+Llama-v1_1745515774) introduces advanced reasoning and vision capabilities tailored for enterprise applications. The Granite Vision 3.2 2B model is a lightweight, open-source vision-language model specifically optimized for visual document understanding. Trained on a comprehensive instruction-following dataset, it excels in extracting information from tables, charts, diagrams, and infographics, making it a powerful tool for structured data analysis in business contexts. \n",
    "\n",
    "In addition to its vision capabilities, Granite 3.2 incorporates enhanced reasoning features. The models support conditional reasoning, allowing users to activate or deactivate reasoning processes as needed, optimizing computational resources. This flexibility enables the models to handle complex decision-making tasks efficiently, such as software engineering challenges and IT issue resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='Setup'></a>[Setup](#toc0_)\n",
    "\n",
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "\n",
    "*   [`ibm-watsonx-ai`](https://pypi.org/project/ibm-watsonx-ai/): `ibm-watsonx-ai` is a library that allows to work with watsonx.ai service on IBM Cloud and IBM Cloud for Data. Train, test and deploy your models as APIs for application development, share with colleagues using this python library.\n",
    "\n",
    "* `image`: `image` from Pillow is the Python Imaging Library (PIL) fork that provides easy-to-use methods for opening, manipulating, and saving image files in various formats. It’s commonly used for preprocessing images before feeding them into machine learning models or APIs.\n",
    "\n",
    "* `requests`: `requests` is a simple and intuitive HTTP library for Python. It sends all kinds of HTTP/1.1 requests with methods like GET and POST. In this lab, it downloads images from the web for analysis by the multimodal AI model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Installing required libraries](#toc0_)\n",
    "\n",
    "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You must run the following cell__ to install them. Please wait until it completes.\n",
    "\n",
    "This step could take **several minutes**; please be patient.\n",
    "\n",
    "**NOTE**: If you encounter any issues, please restart the kernel and run the cell again.  You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/crvBKBOkg9aBzXZiwGEXbw/Restarting-the-Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install ibm-watsonx-ai==1.1.20 image==1.5.33 requests==2.32.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[watsonx API credentials and project_id](#toc0_)\n",
    "\n",
    "\n",
    "This section provides you with the necessary credentials to access the watsonx API.\n",
    "\n",
    "**Please note:**\n",
    "\n",
    "In this lab environment, you don't need to specify the api_key, and the project_id is pre_set as \"skills-network\", but if you want to use the model locally, you need to go to [watsonx](https://www.ibm.com/watsonx?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-Build+an+Image+Captioning+System+with+watsonx+and+Llama-v1_1745515774) to create your own keys and id.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FLAN_T5_XL': 'google/flan-t5-xl', 'FLAN_T5_XXL': 'google/flan-t5-xxl', 'FLAN_UL2': 'google/flan-ul2', 'GRANITE_13B_INSTRUCT_V2': 'ibm/granite-13b-instruct-v2', 'GRANITE_3_2_8B_INSTRUCT': 'ibm/granite-3-2-8b-instruct', 'GRANITE_3_2B_INSTRUCT': 'ibm/granite-3-2b-instruct', 'GRANITE_3_3_8B_INSTRUCT': 'ibm/granite-3-3-8b-instruct', 'GRANITE_3_8B_INSTRUCT': 'ibm/granite-3-8b-instruct', 'GRANITE_8B_CODE_INSTRUCT': 'ibm/granite-8b-code-instruct', 'GRANITE_GUARDIAN_3_2B': 'ibm/granite-guardian-3-2b', 'GRANITE_GUARDIAN_3_8B': 'ibm/granite-guardian-3-8b', 'GRANITE_VISION_3_2_2B': 'ibm/granite-vision-3-2-2b', 'LLAMA_2_13B_CHAT': 'meta-llama/llama-2-13b-chat', 'LLAMA_3_2_11B_VISION_INSTRUCT': 'meta-llama/llama-3-2-11b-vision-instruct', 'LLAMA_3_2_1B_INSTRUCT': 'meta-llama/llama-3-2-1b-instruct', 'LLAMA_3_2_3B_INSTRUCT': 'meta-llama/llama-3-2-3b-instruct', 'LLAMA_3_2_90B_VISION_INSTRUCT': 'meta-llama/llama-3-2-90b-vision-instruct', 'LLAMA_3_3_70B_INSTRUCT': 'meta-llama/llama-3-3-70b-instruct', 'LLAMA_3_405B_INSTRUCT': 'meta-llama/llama-3-405b-instruct', 'LLAMA_4_MAVERICK_17B_128E_INSTRUCT_FP8': 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8', 'LLAMA_GUARD_3_11B_VISION': 'meta-llama/llama-guard-3-11b-vision', 'MISTRAL_LARGE': 'mistralai/mistral-large', 'MISTRAL_MEDIUM_2505': 'mistralai/mistral-medium-2505', 'MISTRAL_SMALL_3_1_24B_INSTRUCT_2503': 'mistralai/mistral-small-3-1-24b-instruct-2503', 'MIXTRAL_8X7B_INSTRUCT_V01': 'mistralai/mixtral-8x7b-instruct-v01', 'PIXTRAL_12B': 'mistralai/pixtral-12b'}\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai import Credentials, APIClient\n",
    "import os\n",
    "\n",
    "credentials = Credentials(\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    )\n",
    "\n",
    "project_id=\"skills-network\"\n",
    "client = APIClient(credentials)\n",
    "# GET TextModels ENUM\n",
    "client.foundation_models.TextModels\n",
    "\n",
    "# PRINT dict of Enums\n",
    "client.foundation_models.TextModels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"#Image-preparation\">Image preparation</a>\n",
    "\n",
    "- Download the image\n",
    "- Display the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_image_1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/5uo16pKhdB1f2Vz7H8Utkg/image-1.png'\n",
    "url_image_2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fsuegY1q_OxKIxNhf6zeYg/image-2.png'\n",
    "url_image_3 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KCh_pM9BVWq_ZdzIBIA9Fw/image-3.png'\n",
    "url_image_4 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VaaYLw52RaykwrE3jpFv7g/image-4.png'\n",
    "\n",
    "image_urls = [url_image_1, url_image_2, url_image_3, url_image_4] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a better understanding of our data input, let's display the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image 1](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/5uo16pKhdB1f2Vz7H8Utkg/image-1.png)<figcaption>Image 1</figcaption>\n",
    "\n",
    "![Image 2](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/fsuegY1q_OxKIxNhf6zeYg/image-2.png)<figcaption>Image 2</figcaption>\n",
    "\n",
    "![Image 3](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/KCh_pM9BVWq_ZdzIBIA9Fw/image-3.png)<figcaption>Image 3</figcaption>\n",
    "\n",
    "![Image 4](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VaaYLw52RaykwrE3jpFv7g/image-4.png)<figcaption>Image 4</figcaption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"#Work-with-large-language-models-on-watsonx.ai\"></a>[Work with large language models on watsonx.ai](#toc0_)\n",
    "\n",
    "Specify the `model_id` of the model that you will use for the chat with image modalities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'ibm/granite-vision-3-2-2b'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_8_1_'></a>[Check the model parameters](#toc0_)\n",
    "\n",
    "More information about the `TextChatParameters` can be found here: [docs](https://ibm.github.io/watsonx-ai-python-sdk/fm_schema.html#ibm_watsonx_ai.foundation_models.schema.TextChatParameters), [source](https://ibm.github.io/watsonx-ai-python-sdk/_modules/ibm_watsonx_ai/foundation_models/schema/_api.html#TextChatParameters).\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class TextChatParameters(BaseSchema):\n",
    "    frequency_penalty: float | None = None\n",
    "    logprobs: bool | None = None\n",
    "    top_logprobs: int | None = None\n",
    "    presence_penalty: float | None = None\n",
    "    response_format: dict | TextChatResponseFormat | None = None\n",
    "    temperature: float | None = None\n",
    "    max_tokens: int | None = None\n",
    "    time_limit: int | None = None\n",
    "    top_p: float | None = None\n",
    "    n: int | None = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_sample_params(cls) -> dict[str, Any]:\n",
    "        \"\"\"Provide example values for TextChatParameters.\"\"\"\n",
    "        return {\n",
    "            \"frequency_penalty\": 0.5,\n",
    "            \"logprobs\": True,\n",
    "            \"top_logprobs\": 3,\n",
    "            \"presence_penalty\": 0.3,\n",
    "            \"response_format\": TextChatResponseFormat.get_sample_params(),\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 100,\n",
    "            \"time_limit\": 600000,\n",
    "            \"top_p\": 0.9,\n",
    "            \"n\": 1,\n",
    "        }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| PARAMETER         | TYPE                                                                              | EXAMPLE VALUE           |\n",
      "+===================+===================================================================================+=========================+\n",
      "| frequency_penalty | float | None                                                                      | 0.5                     |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| logprobs          | bool | None                                                                       | True                    |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| top_logprobs      | int | None                                                                        | 3                       |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| presence_penalty  | float | None                                                                      | 0.3                     |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| response_format   | dict | ibm_watsonx_ai.foundation_models.schema._api.TextChatResponseFormat | None | {'type': 'json_object'} |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| temperature       | float | None                                                                      | 0.7                     |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| max_tokens        | int | None                                                                        | 100                     |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| time_limit        | int | None                                                                        | 600000                  |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| top_p             | float | None                                                                      | 0.9                     |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n",
      "| n                 | int | None                                                                        | 1                       |\n",
      "+-------------------+-----------------------------------------------------------------------------------+-------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextChatParameters(frequency_penalty=None, logprobs=None, top_logprobs=None, presence_penalty=None, response_format=None, temperature=0.2, max_tokens=None, time_limit=None, top_p=0.5, n=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.schema import TextChatParameters\n",
    "\n",
    "TextChatParameters.show()\n",
    "\n",
    "params = TextChatParameters(\n",
    "    temperature=0.2,\n",
    "    top_p=0.5,\n",
    "\n",
    ")\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Initialize the model](#toc0_)\n",
    "\n",
    "\n",
    "Initialize the `ModelInference` class with the previously specified parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "model = ModelInference(\n",
    "    model_id=model_id,\n",
    "    credentials=credentials,\n",
    "    project_id=project_id,\n",
    "    params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Encode the image](#toc0_)\n",
    "\n",
    "Encode the image to `base64.b64encode`. Why do you need to encode the image to `base64.b64encode`? JSON is a text-based format and does not support binary data. By encoding the image as a Base64 string, you can embed the image data directly within the JSON structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def encode_images_to_base64(image_urls):\n",
    "    \"\"\"\n",
    "    Downloads and encodes a list of image URLs to base64 strings.\n",
    "\n",
    "    Parameters:\n",
    "    - image_urls (list): A list of image URLs.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of base64-encoded image strings.\n",
    "    \"\"\"\n",
    "    encoded_images = []\n",
    "    for url in image_urls:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            encoded_image = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "            encoded_images.append(encoded_image)\n",
    "            print(type(encoded_image))\n",
    "        else:\n",
    "            print(f\"Warning: Failed to fetch image from {url} (Status code: {response.status_code})\")\n",
    "            encoded_images.append(None)\n",
    "    return encoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "encoded_images = encode_images_to_base64(image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#Multimodal-inference-function'></a>[Multimodal inference function](#toc0_)\n",
    "\n",
    "Next, define a function to generate responses from the model.\n",
    "\n",
    "The `generate_model_response` function is designed to interact with a multimodal AI model that accepts both text and image inputs. This function takes an image, along with a user’s query, and generates a response from the model.\n",
    "\n",
    "#### Function purpose\n",
    "\n",
    "The function sends an image and a query to the AI model and retrieves a description or answer. It combines a text-based prompt and an image to guide the model in generating a concise response.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "- **`encoded_image`** (`str`): A base64-encoded image string, which allows the model to process the image data.\n",
    "- **`user_query`** (`str`): The user's question about the image, providing context for the model to interpret the image and answer appropriately.\n",
    "- **`assistant_prompt`** (`str`): An optional text prompt to guide the model in responding in a specific way. By default, the prompt is set to: `\"You are a helpful assistant. Answer the following user query in 1 or 2 sentences:\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_response(encoded_image, user_query, assistant_prompt=\"You are a helpful assistant. Answer the following user query in 1 or 2 sentences: \"):\n",
    "    \"\"\"\n",
    "    Sends an image and a query to the model and retrieves the description or answer.\n",
    "\n",
    "    Parameters:\n",
    "    - encoded_image (str): Base64-encoded image string.\n",
    "    - user_query (str): The user's question about the image.\n",
    "    - assistant_prompt (str): Optional prompt to guide the model's response.\n",
    "\n",
    "    Returns:\n",
    "    - str: The model's response for the given image and query.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the messages object\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": assistant_prompt + user_query\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"data:image/jpeg;base64,\" + encoded_image,\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Send the request to the model\n",
    "    response = model.chat(messages=messages)\n",
    "    \n",
    "    # Return the model's response\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps explained\n",
    "\n",
    "1. **Create the Messages object:**  \n",
    "   The function constructs a list of messages in JSON-like format. This object includes:\n",
    "   - A \"user\" role with a \"content\" array. The content array contains:\n",
    "     - A text field, combining the `assistant_prompt` and the `user_query`.\n",
    "     - An image URL field, which includes a base64-encoded image string. This is essential for sending image data to the model.\n",
    "2. **Send the request to the model:** `response = model.chat(messages=messages)`\n",
    "\tThe function sends the constructed messages to the model using a chat-based API. The model.chat function is invoked with the messages parameter to generate the model's response.\n",
    "3. **Return the model’s response:** `return response['choices'][0]['message']['content']`\n",
    "\tThe model’s response is returned as a string, extracted from the response object. Specifically, the function retrieves the content of the first choice in the model's response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#Image-captioning'></a>[Image captioning](#toc0_)\n",
    "\n",
    "Generate an answer to your question using the `ibm/granite-vision-3-2-2b` model.\n",
    "\n",
    "More information about the `chat` can be found here: [docs](https://ibm.github.io/watsonx-ai-python-sdk/fm_model_inference.html#ibm_watsonx_ai.foundation_models.inference.ModelInference.chat), [source](https://ibm.github.io/watsonx-ai-python-sdk/_modules/ibm_watsonx_ai/foundation_models/inference/model_inference.html#ModelInference.chat).\n",
    "\n",
    "Now, you can loop through our images to see the text descriptions produced by the model in response to the query, \"Describe the photo\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description for image 1: The photo captures a bustling urban street scene, likely in a major city given the density of the buildings and the variety of commercial establishments. The architecture is diverse, with a mix of modern high-rises and older, more traditional buildings. The street is well-lit, suggesting it's daytime, and the presence of pedestrians and vehicles indicates a busy time of day, possibly rush hour. The overcast sky suggests it might be a cool day or that the photo was taken during a time when the sun is not at its peak. The image does not provide any specific information about the location or the time of year, but the overall ambiance is one of urban life and activity./n/n\n",
      "Description for image 2: The photo captures a moment of athleticism and determination, showcasing a person in the midst of a run. The individual is dressed in athletic attire suitable for running, with a bright yellow top and black leggings, which stand out against the urban backdrop. The setting appears to be a large, open space, possibly a parking lot or an industrial area, given the presence of a building with a numbered address and a car parked in the background. The lighting suggests it is either early morning or late afternoon, as the sun casts long shadows and the sky is bright but not harsh. The focus on the runner, with the background blurred, emphasizes the motion and energy of the scene./n/n\n",
      "Description for image 3: The photo shows a flooded rural area with agricultural fields on the left and a house on the right. The house is partially submerged, and the surrounding fields are also flooded. There are two large cylindrical structures in the foreground, which could be silos or storage tanks. The water level is high enough to cover the ground and the house, indicating a significant flooding event. The overcast sky suggests that the flooding could be due to heavy rainfall or a nearby water source overflowing. The absence of people or vehicles suggests that the area is not currently inhabited or accessible./n/n\n",
      "Description for image 4: In this image we can see a person's hand and there is a food item./n/n\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Describe the photo\"\n",
    "\n",
    "for i in range(len(encoded_images)):\n",
    "    image = encoded_images[i]\n",
    "\n",
    "    response = generate_model_response(image, user_query)\n",
    "\n",
    "    # Print the response with a formatted description\n",
    "    print(f\"Description for image {i + 1}: {response}/n/n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#Object-detection'></a>[Object detection](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have showcased the model's ability to perform image captioning in the previous step, let's ask the model some questions that require object detection. Our system prompt will remain the same as in the previous section. The difference now will be in the user query. Regarding the second image depicting the woman running outdoors, you will be asking the model, \"How many cars are in this image?\". You can comment out the code section for image captioning if you don't want to wait for the response on that part again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How many cars are in this image?\n",
      "Model Response:  There is one car visible in the image, parked in the background on the right side.\n"
     ]
    }
   ],
   "source": [
    "image = encoded_images[1]\n",
    "\n",
    "user_query = \"How many cars are in this image?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly identified the singular vehicle in the image. Now, let's inquire about the damage depicted in the image of flooding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How severe is the damage in this image?\n",
      "Model Response:  The damage in the image is quite severe. The floodwaters have risen to a level that has submerged the crops, which are typically the most valuable part of a farm. The houses are also at risk, with the water level reaching the base of the walls, suggesting that the water level could rise further. The presence of the silos and the utility pole indicates that the flooding has affected infrastructure as well. The extent of the damage suggests that the area has been affected by a significant flood event, likely causing substantial economic and environmental impacts.\n"
     ]
    }
   ],
   "source": [
    "image = encoded_images[2]\n",
    "\n",
    "user_query = \"How severe is the damage in this image?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This response highlights the value that multimodal AI has for domains like insurance. The model was able to detect the severity of the damage caused to the flooded home. This could be a powerful tool for improving insurance claim processing time.\n",
    "\n",
    "Next, let's ask the model how much sodium content is in the nutrition label image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How much sodium is in this product?\n",
      "Model Response:  The nutrition facts label on the product indicates that it contains 640mg of sodium. This amount is listed under the \"Sodium\" category, which is a common source of sodium in processed foods.\n"
     ]
    }
   ],
   "source": [
    "image = encoded_images[3]\n",
    "\n",
    "user_query = \"How much sodium is in this product?\"\n",
    "\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The model was able to discern objects within the images following user queries. We encourage you to try more queries to further demonstrate the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#Conclusion'></a>[Conclusion](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you explored the capabilities of IBM's Granite 3.2 Vision, one of the latest and most powerful multimodal models developed by IBM. Using IBM watsonx, you were able to seamlessly integrate this model into a Python-based workflow to perform tasks like:\n",
    "\n",
    "- Generating detailed image captions\n",
    "\n",
    "- Answering object detection questions (e.g., number of cars in an image)\n",
    "\n",
    "- Assessing visual damage in real-world disaster scenarios\n",
    "\n",
    "- Extracting specific information from product labels\n",
    "\n",
    "This lab not only introduced you to multimodal AI development but also demonstrated how cutting-edge models can turn visual content into actionable insight. Whether you're building apps for enterprise, education, or everyday use, the tools and techniques you’ve learned here are a solid foundation for what's possible with AI today.\n",
    "\n",
    "We encourage you to extend this notebook by asking new questions, uploading your own images, or combining image and text prompts for more advanced reasoning tasks. The future of AI is multimodal—this is your starting point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='#Exercises'></a>[Exercises](#toc0_)\n",
    "\n",
    "Now, let's practice by exploring some other capabilities of this model. Try asking \"How much cholesterol is in this product?\" in the 4th image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query:  How much cholesterol is in this product?\n",
      "Model Response:  The product contains 0 grams of cholesterol, as indicated on the nutrition facts label.\n"
     ]
    }
   ],
   "source": [
    "image = encoded_images[3]\n",
    "user_query = \"How much cholesterol is in this product?\"\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution\n",
    "    </summary>\n",
    "\n",
    "```python\n",
    "image = encoded_images[3]\n",
    "user_query = \"How much cholesterol is in this product?\"\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try asking \"What is the color of the woman's jacket?\" in the 2nd image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user query is :: what is the color of the woman's Jacet?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gernerate_model_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m user_q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the color of the woman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Jacet?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe user query is ::\u001b[39m\u001b[38;5;124m\"\u001b[39m,user_q)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel_prediction is :\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[43mgernerate_model_response\u001b[49m(image,user_q))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gernerate_model_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "image=encoded_images[1]\n",
    "user_q=\"what is the color of the woman's Jacet?\"\n",
    "print(\"The user query is ::\",user_q)\n",
    "print(\"Model_prediction is :\",gernerate_model_response(image,user_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution\n",
    "    </summary>\n",
    "\n",
    "```python\n",
    "image = encoded_images[1]\n",
    "user_query = \"What is the color of the woman's jacket?\"\n",
    "print(\"User Query: \", user_query)\n",
    "print(\"Model Response: \", generate_model_response(image, user_query))\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hailey Quach](https://www.haileyq.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change log\n",
    "\n",
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2025-04-24|1.0|Hailey Quach|Ininitial version|\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "b631c1541313ee4291baea188fd6b6d2f44bf4a8312116e4621f0bec039e5317"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
